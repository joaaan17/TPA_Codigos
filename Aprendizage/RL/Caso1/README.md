# ğŸ§  Reinforcement Learning - VisualizaciÃ³n Web

Sistema completo de Reinforcement Learning con visualizaciÃ³n interactiva en el navegador.

## ğŸ“ Estructura del Proyecto

```
RL/
â”œâ”€â”€ index.html              # PÃ¡gina principal de navegaciÃ³n
â”œâ”€â”€ random-agent.html       # DemostraciÃ³n de agente aleatorio
â”œâ”€â”€ q-learning.html         # Entrenamiento con Q-Learning
â”œâ”€â”€ sarsa.html             # Entrenamiento con SARSA
â”œâ”€â”€ README.md              # Este archivo
â”œâ”€â”€ core/                  # CÃ³digo JavaScript core
â”‚   â”œâ”€â”€ env_2D.js         # Entorno 2D (replica env_2D.py)
â”‚   â””â”€â”€ agentesRL.js      # Algoritmos RL (replica agentesRL.py)
â”œâ”€â”€ scripts/              # Scripts de control
â”‚   â”œâ”€â”€ random-agent.js   # LÃ³gica del agente aleatorio
â”‚   â””â”€â”€ training-common.js # LÃ³gica comÃºn de entrenamiento
â””â”€â”€ styles/               # Estilos CSS
    â””â”€â”€ common.css        # Estilos compartidos
```

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Abrir directamente
1. Abre `index.html` en tu navegador
2. Selecciona la demo que quieras probar
3. Â¡Listo!

### OpciÃ³n 2: Demos individuales
- **Agente Aleatorio:** Abre `random-agent.html`
- **Q-Learning:** Abre `q-learning.html`
- **SARSA:** Abre `sarsa.html`

## ğŸ“š Demos Disponibles

### 1. ğŸ² Agente Aleatorio
**Archivo:** `random-agent.html`

Observa un agente que se mueve aleatoriamente en el entorno.

**CaracterÃ­sticas:**
- Movimientos completamente aleatorios
- Control de velocidad
- Modo paso a paso
- VisualizaciÃ³n en tiempo real

**Uso:**
1. Configura el tamaÃ±o del entorno y obstÃ¡culos
2. Presiona "Iniciar" para comenzar la simulaciÃ³n
3. Observa cÃ³mo el agente explora sin estrategia

**Equivalente Python:**
```python
action = random.choice(env.get_valid_actions())
env.step(action)
```

### 2. ğŸ“ Q-Learning
**Archivo:** `q-learning.html`

Entrena un agente con el algoritmo Q-Learning.

**CaracterÃ­sticas:**
- Off-policy TD control
- ParÃ¡metros configurables (Î±, Î³, Îµ)
- GrÃ¡fico de aprendizaje en tiempo real
- VisualizaciÃ³n de Q-table
- Testing del agente entrenado

**ParÃ¡metros:**
- **Alpha (Î±):** Learning rate (0.1 recomendado)
- **Gamma (Î³):** Discount factor (0.9 recomendado)
- **Epsilon (Îµ):** Exploration rate (0.1 recomendado)
- **Episodios:** NÃºmero de episodios de entrenamiento

**Algoritmo:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
```

### 3. ğŸ“š SARSA
**Archivo:** `sarsa.html`

Entrena un agente con el algoritmo SARSA.

**CaracterÃ­sticas:**
- On-policy TD control
- ParÃ¡metros configurables
- ComparaciÃ³n con Q-Learning
- VisualizaciÃ³n de polÃ­tica

**ParÃ¡metros:** Mismos que Q-Learning

**Algoritmo:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```

**Diferencia con Q-Learning:**
- Q-Learning usa `max Q(s',a')` (off-policy)
- SARSA usa `Q(s',a')` donde a' es la acciÃ³n que realmente tomarÃ¡ (on-policy)

## ğŸ¯ Flujo de Aprendizaje Recomendado

1. **Comenzar con Agente Aleatorio**
   - Entender el problema
   - Ver cÃ³mo se comporta sin aprendizaje
   - Notar la ineficiencia

2. **Experimentar con Q-Learning**
   - Configurar parÃ¡metros bÃ¡sicos
   - Entrenar 1000 episodios
   - Observar el grÃ¡fico de aprendizaje
   - Probar el agente entrenado

3. **Comparar con SARSA**
   - Usar las mismas configuraciones
   - Entrenar y comparar resultados
   - Analizar diferencias

4. **Experimentar**
   - Variar parÃ¡metros (Î±, Î³, Îµ)
   - Cambiar tamaÃ±o del entorno
   - AÃ±adir mÃ¡s obstÃ¡culos
   - Observar cÃ³mo afecta el aprendizaje

## âš™ï¸ ConfiguraciÃ³n del Entorno

### ParÃ¡metros BÃ¡sicos
- **Ancho/Alto:** 5-20 (10 recomendado para empezar)
- **ObstÃ¡culos:** 0-40% (10% recomendado)

### ParÃ¡metros del Agente
- **Alpha (Î±):** 0.01-0.5
  - Muy bajo: Aprendizaje lento
  - Muy alto: Inestabilidad
  - Recomendado: 0.1

- **Gamma (Î³):** 0.7-0.99
  - Bajo: Miope (solo importa recompensa inmediata)
  - Alto: Visionario (considera futuro lejano)
  - Recomendado: 0.9

- **Epsilon (Îµ):** 0.01-0.5
  - Bajo: MÃ¡s explotaciÃ³n
  - Alto: MÃ¡s exploraciÃ³n
  - Recomendado: 0.1

## ğŸ“Š Interpretando los Resultados

### GrÃ¡fico de Recompensas
- **Tendencia ascendente:** El agente estÃ¡ aprendiendo
- **Meseta:** El agente ha convergido
- **Oscilaciones:** Puede necesitar mÃ¡s episodios o ajustar Î±

### MÃ©tricas
- **Pasos:** Menos pasos = Mejor polÃ­tica aprendida
- **Recompensa total:** Mayor recompensa = Mejor desempeÃ±o
- **Promedio (Ãºltimos 100):** Indica estabilidad del aprendizaje

## âŒ¨ï¸ Atajos de Teclado

### Agente Aleatorio
- `Espacio`: Iniciar/Pausar
- `R`: Reiniciar
- `N`: Nuevo entorno
- `S`: Paso a paso

### Q-Learning / SARSA
- `Ctrl+E`: Crear entorno
- `Ctrl+T`: Iniciar entrenamiento
- `Ctrl+S`: Detener entrenamiento

## ğŸ”§ CaracterÃ­sticas TÃ©cnicas

### Environment2D
- Grid configurable
- ObstÃ¡culos aleatorios
- Sistema de recompensas (-1 por paso, +1 al objetivo)
- Rendering en HTML5 Canvas

### Agent
- ImplementaciÃ³n de Q-Learning y SARSA
- Epsilon-greedy para exploraciÃ³n/explotaciÃ³n
- Q-table con inicializaciÃ³n a ceros
- MÃ©todos de anÃ¡lisis (estadÃ­sticas, polÃ­tica)

### VisualizaciÃ³n
- Canvas HTML5 para rendering del entorno
- GrÃ¡ficos de recompensas en tiempo real
- Log de eventos
- Interfaz responsive

## ğŸ“ Conceptos Clave

### Q-Table
Tabla que almacena el valor estimado de cada par (estado, acciÃ³n).

### Epsilon-Greedy
Estrategia de exploraciÃ³n:
- Con probabilidad Îµ: acciÃ³n aleatoria (exploraciÃ³n)
- Con probabilidad 1-Îµ: mejor acciÃ³n conocida (explotaciÃ³n)

### Temporal Difference (TD)
ActualizaciÃ³n incremental basada en estimaciones:
- No requiere esperar al final del episodio
- Aprende de cada transiciÃ³n

### Off-Policy vs On-Policy
- **Off-policy (Q-Learning):** Aprende polÃ­tica Ã³ptima independientemente de la polÃ­tica seguida
- **On-policy (SARSA):** Aprende la polÃ­tica que realmente sigue

## ğŸ› SoluciÃ³n de Problemas

### El agente no aprende
- Aumentar nÃºmero de episodios
- Verificar que Î± > 0
- Aumentar Îµ para mÃ¡s exploraciÃ³n
- Reducir obstÃ¡culos

### El grÃ¡fico oscila mucho
- Reducir Î± (learning rate)
- Aumentar nÃºmero de episodios
- Reducir Îµ despuÃ©s de cierto entrenamiento

### El navegador se congela
- Desactivar "Visualizar" durante entrenamiento
- Reducir nÃºmero de episodios
- Usar entorno mÃ¡s pequeÃ±o

## ğŸ’¾ Guardar/Cargar Agentes

Los agentes entrenados se pueden exportar/importar:

```javascript
// Exportar
const qdata = agent.exportQTable();
localStorage.setItem('myAgent', JSON.stringify(qdata));

// Importar
const qdata = JSON.parse(localStorage.getItem('myAgent'));
agent.importQTable(qdata);
```

## ğŸ“ Correspondencia con Python

| Python | JavaScript |
|--------|-----------|
| `env_2D.py` | `core/env_2D.js` |
| `agentesRL.py` | `core/agentesRL.js` |
| `ej_alumn.py` | `random-agent.html` |
| `agent.train_q_learning()` | `agent.trainQLearning()` |
| `agent.train_sarsa()` | `agent.trainSARSA()` |
| `agent.test_agent()` | `agent.testAgent()` |

## ğŸŒŸ CaracterÃ­sticas Avanzadas

### AnÃ¡lisis de Q-Table
```javascript
// Ver polÃ­tica aprendida
agent.printPolicy();

// EstadÃ­sticas
const stats = agent.getQTableStats();
console.log(stats);
// { mean, min, max, nonZeroCount, totalValues, sparsity }

// Mejor acciÃ³n para un estado
const bestAction = agent.getBestAction([0, 0]);
```

### Callbacks Personalizados
Los mÃ©todos de entrenamiento aceptan callbacks:

```javascript
agent.trainQLearning(1000, 
    (episode, total) => console.log(`Episodio ${episode}/${total}`),
    (episode, reward, rewards) => updateChart(rewards)
);
```

## ğŸ“– Recursos Adicionales

- **Sutton & Barto:** "Reinforcement Learning: An Introduction"
- **David Silver:** Curso de RL (YouTube)
- **OpenAI Spinning Up:** spinning-up.openai.com

## ğŸ¤ Contribuciones

Este proyecto es para fines acadÃ©micos del curso TPA.

## ğŸ“„ Licencia

Uso acadÃ©mico - TÃ©cnicas de ProgramaciÃ³n Avanzada

---

**ğŸ’¡ Tip Final:** Experimenta con diferentes configuraciones y observa cÃ³mo afectan el aprendizaje. Â¡La mejor manera de aprender es practicando!


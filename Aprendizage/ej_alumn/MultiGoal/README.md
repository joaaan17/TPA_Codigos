# ğŸ¯ Sistema de Aprendizaje Multi-Goal

## ğŸ“‹ Archivos Creados

### VersiÃ³n Single-Goal (Original)
- `training.html` - Interfaz web original
- `training.js` - LÃ³gica de entrenamiento
- `env_2D.js` - Entorno con 1 meta
- `agentesRL.js` - Agente RL estÃ¡ndar

### VersiÃ³n Multi-Goal (Nueva) â­
- `trainingMulti.html` - Interfaz web multi-goal
- `trainingMulti.js` - LÃ³gica de entrenamiento multi-goal
- `env_2DMulti.js` - Entorno con mÃºltiples metas
- `agentesRLMulti.js` - Agente RL multi-goal

---

## ğŸ†š Diferencias Principales

### 1. **Entorno (`env_2DMulti.js`)**

#### Single-Goal:
```javascript
this.goal = [width - 1, height - 1]; // 1 meta fija
reward = 1; // Recompensa: +1
```

#### Multi-Goal:
```javascript
this.goals = []; // Array de mÃºltiples metas
this.numGoals = numGoals; // Configurable
reward = 10; // Recompensa mayor: +10
```

**CaracterÃ­sticas Multi-Goal:**
- âœ… MÃºltiples metas generadas aleatoriamente
- âœ… El agente termina al alcanzar CUALQUIER meta
- âœ… Cada meta tiene un color diferente (estrella con nÃºmero)
- âœ… Recompensa mÃ¡s alta (+10 vs +1) para compensar dificultad

---

### 2. **VisualizaciÃ³n**

#### Single-Goal:
- ğŸ”´ 1 cÃ­rculo rojo (meta Ãºnica)
- Esquina inferior derecha

#### Multi-Goal:
- â­ MÃºltiples estrellas de colores
- ğŸ¨ Colores: Oro, Rojo, Verde, Rosa, Cyan, Naranja
- ğŸ”¢ Cada estrella tiene un nÃºmero (1, 2, 3...)
- ğŸ“ Posiciones aleatorias

---

### 3. **PolÃ­tica de Aprendizaje**

#### Frozen Policy (Single-Goal):
```
PolÃ­tica determinista â†’ 1 meta fija
El agente siempre va al mismo destino
```

#### Multi-Goal Policy:
```
PolÃ­tica flexible â†’ n metas posibles
El agente aprende a llegar a la META MÃS CERCANA
Generaliza mejor â†’ mÃ¡s robusto
```

---

## ğŸ§  Â¿Por QuÃ© Multi-Goal?

### Ventajas:
1. **GeneralizaciÃ³n**: El agente aprende a navegar a mÃºltiples destinos
2. **Robustez**: No depende de una Ãºnica configuraciÃ³n
3. **ExploraciÃ³n**: Fomenta mejor exploraciÃ³n del espacio de estados
4. **Realismo**: MÃ¡s parecido a problemas del mundo real

### Desventajas:
- Mayor complejidad
- Puede tardar mÃ¡s en converger
- Necesita mÃ¡s episodios de entrenamiento

---

## ğŸš€ CÃ³mo Usar

### 1. Abrir la Interfaz
```
Abre: trainingMulti.html
```

### 2. Configurar ParÃ¡metros
- **NÃºmero de Metas**: 2-8 (recomendado: 3-5)
- **TamaÃ±o Grid**: 10x10 (default)
- **ObstÃ¡culos**: 10% (default)
- **Episodios**: 1000+ (multi-goal necesita mÃ¡s)

### 3. Crear Entorno
- Click en "Crear Entorno"
- Se generarÃ¡n las metas aleatoriamente

### 4. Entrenar
- Selecciona algoritmo (Q-Learning o SARSA)
- Click en "Entrenar"
- Observa las estadÃ­sticas de distribuciÃ³n de metas

### 5. Probar
- Click en "Probar Agente"
- VerÃ¡s a quÃ© metas llega mÃ¡s frecuentemente

---

## ğŸ“Š InterpretaciÃ³n de Resultados

### Single-Goal:
```
Meta alcanzada: 100% de las veces en la misma posiciÃ³n
Recompensa tÃ­pica: -17 (grid 10x10)
```

### Multi-Goal:
```
Meta alcanzada: DistribuciÃ³n entre las n metas
Recompensa tÃ­pica: -8 a +5 (llega mÃ¡s rÃ¡pido a meta cercana)
DistribuciÃ³n: [30%, 25%, 20%, 15%, 10%] (ejemplo 5 metas)
```

---

## ğŸ”¬ ComparaciÃ³n Experimental

### ConfiguraciÃ³n de Prueba:
- Grid: 10x10
- ObstÃ¡culos: 10%
- Episodios: 1000
- Alpha: 0.1, Gamma: 0.9, Epsilon: 0.1

### Resultados Esperados:

| MÃ©trica | Single-Goal | Multi-Goal (3 metas) |
|---------|-------------|----------------------|
| Convergencia | ~300 episodios | ~500 episodios |
| Recompensa Promedio | -15 a -20 | -5 a -10 |
| ExploraciÃ³n | Baja | Alta |
| Robustez | Baja | Alta |

---

## ğŸ’¡ Conceptos Clave

### Frozen Policy:
- PolÃ­tica aprendida y **no cambia** despuÃ©s del entrenamiento
- En single-goal: siempre va al mismo lugar
- En multi-goal: **elige dinÃ¡micamente** la mejor meta segÃºn posiciÃ³n

### Multi-Goal Learning:
- El agente aprende valores Q para **alcanzar cualquier meta**
- La Q-Table generaliza: Q(s,a) es Ãºtil para mÃºltiples objetivos
- PolÃ­tica emergente: "ir hacia la meta mÃ¡s cercana/accesible"

---

## ğŸ¯ Casos de Uso

### Single-Goal:
- âœ… Problema especÃ­fico con destino Ãºnico
- âœ… NavegaciÃ³n punto a punto
- âœ… Entorno estÃ¡tico

### Multi-Goal:
- âœ… Robot que debe llegar a estaciones de carga
- âœ… Agente de delivery con mÃºltiples destinos
- âœ… NavegaciÃ³n en entornos dinÃ¡micos
- âœ… Problemas de optimizaciÃ³n de rutas

---

## ğŸ› Debugging

### Si el agente no aprende:
1. Aumenta episodios (multi-goal necesita mÃ¡s)
2. Ajusta epsilon (mÃ¡s exploraciÃ³n)
3. Reduce nÃºmero de metas (empezar con 2-3)
4. Verifica que no haya demasiados obstÃ¡culos

### Si todas las metas tienen distribuciÃ³n similar:
âœ… Â¡Eso es bueno! Significa que el agente aprendiÃ³ a alcanzar cualquiera

### Si una meta domina (>80%):
âš ï¸ Puede ser que esa meta estÃ© mÃ¡s cerca del inicio
ğŸ’¡ Esto es natural y esperado

---

## ğŸ“š Referencias TeÃ³ricas

### Q-Learning Multi-Goal:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
```
Donde `r = 10` si alcanza cualquier meta

### SARSA Multi-Goal:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·Q(s',a') - Q(s,a)]
```
MÃ¡s conservador, aprende polÃ­tica real

---

## ğŸ¨ PersonalizaciÃ³n

### Cambiar colores de metas:
Edita en `env_2DMulti.js`:
```javascript
const goalColors = ['#FFD700', '#FF6347', '#32CD32', ...];
```

### Cambiar recompensa por meta:
```javascript
reward = 10; // Cambia este valor
```

### Agregar mÃ¡s metas:
```html
<input type="number" id="numGoals" value="3" min="2" max="8">
```

---

## âœ… Checklist de ImplementaciÃ³n

- [x] Entorno con mÃºltiples metas
- [x] VisualizaciÃ³n con estrellas de colores
- [x] EstadÃ­sticas de distribuciÃ³n de metas
- [x] Interfaz con configuraciÃ³n de nÃºmero de metas
- [x] Agente que aprende a alcanzar cualquier meta
- [x] Sistema de recompensas ajustado
- [x] Logs informativos sobre quÃ© meta se alcanzÃ³
- [x] Pruebas que muestran distribuciÃ³n

---

## ğŸš€ PrÃ³ximos Pasos

1. **Hindsight Experience Replay (HER)**: Mejorar aprendizaje
2. **Goal Prioritization**: Aprender a priorizar metas
3. **Dynamic Goals**: Metas que cambian durante el entrenamiento
4. **Multi-Agent Multi-Goal**: MÃºltiples agentes compartiendo metas

---

**Autor**: Sistema de Aprendizaje por Refuerzo  
**Fecha**: 2024  
**VersiÃ³n**: 1.0  

Â¡Experimenta y aprende! ğŸ¯ğŸ¤–


# üéÆ Reinforcement Learning - Versi√≥n Web

Este directorio contiene la implementaci√≥n en JavaScript de los algoritmos de Reinforcement Learning del curso.

## üìÅ Archivos

### 1. **index.html** - Simulaci√≥n con Movimientos Aleatorios
Replica el comportamiento de `ej_alumn.py`:
- Agente que se mueve aleatoriamente hasta llegar al objetivo
- Visualizaci√≥n en tiempo real
- Control de velocidad de simulaci√≥n
- Modo paso a paso

**Uso:** Abre `index.html` en tu navegador

### 2. **training.html** - Entrenamiento de Agentes RL
Interfaz completa para entrenar y probar agentes:
- **Algoritmos:** Q-Learning y SARSA
- **Entrenamiento:** Configurable (Œ±, Œ≥, Œµ, episodios)
- **Visualizaci√≥n:** Gr√°fico de recompensas en tiempo real
- **Testing:** Prueba el agente entrenado

**Uso:** Abre `training.html` en tu navegador

### 3. **env_2D.js** - Clase Environment2D
Implementaci√≥n del entorno 2D con:
- Grid configurable
- Obst√°culos aleatorios
- Sistema de recompensas
- Rendering en canvas

### 4. **agentesRL.js** - Clase Agent
Implementaci√≥n de algoritmos RL:
- **Q-Learning:** Temporal Difference Learning
- **SARSA:** On-policy TD Control
- **M√©todos auxiliares:** exportar/importar Q-table, estad√≠sticas, pol√≠tica

### 5. **ej_alumn.js** - L√≥gica de simulaci√≥n aleatoria
Control de la simulaci√≥n con movimientos aleatorios

### 6. **training.js** - L√≥gica de entrenamiento
Control del entrenamiento y testing de agentes

## üöÄ Inicio R√°pido

1. **Demo R√°pida (Movimientos Aleatorios):**
   ```
   Abre: index.html
   Presiona: "Iniciar" para ver al agente moverse aleatoriamente
   ```

2. **Entrenar un Agente:**
   ```
   Abre: training.html
   Configura par√°metros (o usa los por defecto)
   Presiona: "Entrenar"
   Espera a que termine
   Presiona: "Probar Agente"
   ```

## üéØ Funcionalidades Principales

### Environment2D
- Grids de 5x5 hasta 30x30
- Porcentaje configurable de obst√°culos (0-50%)
- Posici√≥n inicial: (0, 0)
- Objetivo: esquina inferior derecha
- Recompensas: -1 por paso, +1 al llegar al objetivo

### Agent (Q-Learning/SARSA)
- **Alpha (Œ±):** Learning rate (0.1 por defecto)
- **Gamma (Œ≥):** Discount factor (0.9 por defecto)
- **Epsilon (Œµ):** Exploration rate (0.1 por defecto)
- **Episodios:** N√∫mero de episodios de entrenamiento

## ‚å®Ô∏è Atajos de Teclado

### index.html
- `Espacio`: Iniciar/Pausar simulaci√≥n
- `R`: Reiniciar
- `N`: Nuevo entorno
- `S`: Paso a paso

### training.html
- `Ctrl+E`: Crear entorno
- `Ctrl+T`: Iniciar entrenamiento
- `Ctrl+S`: Detener entrenamiento

## üìä Caracter√≠sticas Adicionales

### Visualizaci√≥n
- Canvas HTML5 para rendering
- Colores: Azul (agente), Rojo (objetivo), Negro (obst√°culos)
- Grid visible con l√≠neas

### Estad√≠sticas en Tiempo Real
- Posici√≥n actual del agente
- N√∫mero de pasos
- Recompensa total y promedio
- Progreso del entrenamiento

### Gr√°ficos
- Gr√°fico de recompensas por episodio
- Promedio m√≥vil (√∫ltimos 100 episodios)
- Actualizaci√≥n en tiempo real durante el entrenamiento

### Debugging
- Log en consola y en pantalla
- Visualizaci√≥n de pol√≠tica aprendida
- Estad√≠sticas de Q-table
- Exportar/Importar Q-table

## üîß Requisitos

- Navegador web moderno (Chrome, Firefox, Edge, Safari)
- JavaScript habilitado
- No requiere servidor web (funciona con file://)

## üìù Notas

- La Q-table se guarda en memoria (se pierde al recargar la p√°gina)
- Para entrenamientos largos, desactiva "Visualizar" para mejor rendimiento
- Los algoritmos son id√©nticos a la versi√≥n Python

## üéì Uso Acad√©mico

Este c√≥digo replica exactamente el comportamiento de:
- `env_2D.py` ‚Üí `env_2D.js`
- `agentesRL.py` ‚Üí `agentesRL.js`
- `ej_alumn.py` ‚Üí `index.html + ej_alumn.js`

Ideal para:
- Visualizar el comportamiento de los algoritmos
- Experimentar con diferentes par√°metros
- Entender Q-Learning y SARSA visualmente
- Comparar resultados con la versi√≥n Python


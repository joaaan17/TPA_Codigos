# ğŸ”ï¸ Mountain Car - Reinforcement Learning

ImplementaciÃ³n interactiva del clÃ¡sico problema de Reinforcement Learning **Mountain Car** en HTML5 y JavaScript.

## âš¡ OptimizaciÃ³n de Entrenamiento

**NUEVO**: Entrenamiento optimizado sin visualizaciÃ³n durante el proceso. El entrenamiento de 1000 episodios ahora toma solo **5-10 segundos** en lugar de varios minutos. La visualizaciÃ³n solo se muestra al probar el agente entrenado.

## ğŸ“‹ DescripciÃ³n

El **Mountain Car** es un problema paradigmÃ¡tico en el aprendizaje por refuerzo donde un coche atrapado en un valle debe aprender a alcanzar la cima de la colina derecha, a pesar de no tener suficiente potencia para subir directamente.

### ğŸ¯ Objetivo

El coche debe llegar a la cima de la colina derecha (posiciÃ³n â‰¥ 0.5). El desafÃ­o radica en que debe primero retroceder hacia la colina izquierda para ganar impulso y luego acelerar hacia la derecha.

## ğŸ® CaracterÃ­sticas

- **Control Manual**: Prueba el entorno manualmente usando los botones de control
- **Entrenamiento AutomÃ¡tico**: Entrena agentes usando Q-Learning o SARSA
- **VisualizaciÃ³n en Tiempo Real**: Observa cÃ³mo aprende el agente
- **GrÃ¡ficos de Progreso**: Visualiza el aprendizaje con grÃ¡ficos de recompensas
- **ConfiguraciÃ³n Flexible**: Ajusta hiperparÃ¡metros del aprendizaje

## ğŸ§  Algoritmos Implementados

### Q-Learning (Off-Policy)
Algoritmo de Temporal Difference que aprende la polÃ­tica Ã³ptima independientemente de la polÃ­tica seguida durante el entrenamiento.

**ActualizaciÃ³n**:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
```

### SARSA (On-Policy)
Algoritmo que aprende el valor de las acciones basÃ¡ndose en la polÃ­tica que realmente sigue el agente.

**ActualizaciÃ³n**:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```

## âš™ï¸ ParÃ¡metros del Entorno

### Espacio de Estados (Continuo)
- **PosiciÃ³n**: [-1.2, 0.6]
- **Velocidad**: [-0.07, 0.07]

*Nota: Los estados se discretizan en 20x20 bins para usar Q-Learning tabular*

### Espacio de Acciones (Discreto)
- **0**: Acelerar hacia la izquierda
- **1**: No acelerar (neutral)
- **2**: Acelerar hacia la derecha

### Recompensas
- Cada paso: **-1** (penalizaciÃ³n por tiempo)
- Meta alcanzada: Episodio termina
- LÃ­mite de pasos: **200**

### FÃ­sica del Entorno
```javascript
velocity_t+1 = velocity_t + force * power - cos(3 * position_t) * gravity
position_t+1 = position_t + velocity_t+1

donde:
- power = 0.001
- gravity = 0.0025
- force = action - 1  // -1, 0, o +1
```

## ğŸš€ Uso

### 1. Control Manual
Usa los botones de control manual para experimentar con el entorno:
- **â† Izquierda**: Acelera el coche hacia la izquierda
- **â¸ Neutral**: Sin aceleraciÃ³n
- **Derecha â†’**: Acelera el coche hacia la derecha

### 2. Entrenamiento AutomÃ¡tico

1. **Selecciona el algoritmo**: Q-Learning o SARSA
2. **Ajusta los hiperparÃ¡metros**:
   - **Alpha (Î±)**: Tasa de aprendizaje (0.1 recomendado)
   - **Gamma (Î³)**: Factor de descuento (0.99 recomendado)
   - **Epsilon (Îµ)**: Tasa de exploraciÃ³n inicial (1.0)
   - **Decay**: Factor de decaimiento de epsilon (0.995)
   - **Min Îµ**: Epsilon mÃ­nimo (0.01)
3. **Configura episodios**: NÃºmero de episodios de entrenamiento (500-1000 recomendado)
4. **Haz clic en "ğŸ“ Entrenar"**
5. **Observa el progreso** en el grÃ¡fico y logs

### 3. Probar Agente Entrenado

DespuÃ©s del entrenamiento:
1. Haz clic en **"ğŸ§ª Probar Agente"**
2. Observa cÃ³mo el agente usa la polÃ­tica aprendida (sin exploraciÃ³n)

## ğŸ“Š InterpretaciÃ³n de Resultados

### Recompensas
- **Recompensa > -200**: Muy bueno (alcanza la meta rÃ¡pidamente)
- **Recompensa â‰ˆ -150**: Bueno (alcanza la meta)
- **Recompensa = -200**: Malo (no alcanza la meta)

### Tasa de Ã‰xito
- Porcentaje de episodios donde el agente alcanza la meta
- Una buena polÃ­tica deberÃ­a lograr >90% despuÃ©s de suficiente entrenamiento

### Curva de Aprendizaje
- La recompensa promedio debe **aumentar** (menos negativa) con el tiempo
- Indica que el agente estÃ¡ aprendiendo una mejor polÃ­tica

## ğŸ§ª Experimentos Sugeridos

1. **Comparar Q-Learning vs SARSA**
   - Â¿CuÃ¡l converge mÃ¡s rÃ¡pido?
   - Â¿CuÃ¡l encuentra mejor polÃ­tica?

2. **Efecto de Alpha**
   - Alpha alto (0.5): Aprendizaje rÃ¡pido pero inestable
   - Alpha bajo (0.01): Aprendizaje lento pero estable

3. **Efecto de Gamma**
   - Gamma alto (0.99): Considera recompensas futuras
   - Gamma bajo (0.5): Enfoque en recompensas inmediatas

4. **Estrategia de ExploraciÃ³n**
   - Decaimiento rÃ¡pido de epsilon: Convergencia rÃ¡pida pero subÃ³ptima
   - Decaimiento lento: Mejor exploraciÃ³n, convergencia mÃ¡s lenta

## ğŸ’¡ Conceptos Clave de RL

### Sparse Rewards (Recompensas Escasas)
El agente solo recibe seÃ±ales de recompensa significativas al final (alcanzar la meta). Esto hace que el aprendizaje sea mÃ¡s difÃ­cil.

### Delayed Gratification (GratificaciÃ³n Diferida)
El agente debe aprender que alejarse temporalmente del objetivo (retroceder) es necesario para alcanzarlo eventualmente.

### Exploration vs Exploitation
- **ExploraciÃ³n**: Probar acciones aleatorias para descubrir nuevas estrategias
- **ExplotaciÃ³n**: Usar el conocimiento actual para maximizar recompensas

### Credit Assignment Problem
Â¿QuÃ© acciones fueron responsables del Ã©xito/fracaso? El algoritmo TD learning (Q-Learning/SARSA) resuelve esto propagando valores hacia atrÃ¡s.

## ğŸ—ï¸ Estructura del Proyecto

```
MountainCar/
â”œâ”€â”€ index.html          # PÃ¡gina principal con UI
â”œâ”€â”€ styles.css          # Estilos CSS
â”œâ”€â”€ mountaincar.js      # ImplementaciÃ³n del entorno
â”œâ”€â”€ agent.js           # Agentes RL (Q-Learning, SARSA)
â”œâ”€â”€ training.js        # Sistema de entrenamiento
â””â”€â”€ README.md          # Esta documentaciÃ³n
```

## ğŸ“ Recursos Adicionales

- **Sutton & Barto**: "Reinforcement Learning: An Introduction" - CapÃ­tulo 6
- **OpenAI Gymnasium**: [MountainCar-v0 Documentation](https://gymnasium.farama.org/environments/classic_control/mountain_car/)
- **Original Paper**: Moore, A. W. (1990). Efficient Memory-based Learning for Robot Control

## ğŸ› Troubleshooting

**El agente no aprende:**
- Aumenta el nÃºmero de episodios (1000+)
- Verifica que epsilon decay no sea muy rÃ¡pido
- Intenta diferentes tasas de aprendizaje (alpha)

**Entrenamiento muy lento:**
- âœ… **YA OPTIMIZADO**: VisualizaciÃ³n desactivada durante entrenamiento
- El entrenamiento ahora es muy rÃ¡pido (5-10 segundos para 1000 episodios)
- Solo se visualiza al usar "Probar Agente"

**Resultados inconsistentes:**
- El aprendizaje RL es estocÃ¡stico por naturaleza
- Ejecuta mÃºltiples entrenamientos y promedia resultados
- Ajusta la semilla aleatoria (no implementado, pero puede agregarse)

## ğŸ“ Notas de ImplementaciÃ³n

- Estados discretizados en grilla 20x20 (400 estados totales)
- Tabla Q implementada con JavaScript Map
- Epsilon-greedy para balance exploraciÃ³n/explotaciÃ³n
- Canvas HTML5 para renderizado
- AnimaciÃ³n con requestAnimationFrame

## ğŸ¤ Contribuciones

Este proyecto es educativo. SiÃ©ntete libre de experimentar y modificar el cÃ³digo para aprender mÃ¡s sobre Reinforcement Learning.

---

**Â¡DiviÃ©rtete aprendiendo Reinforcement Learning! ğŸ‰**


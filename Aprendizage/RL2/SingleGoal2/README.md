# â„ï¸ Frozen Lake (El Lago Congelado)

## ğŸ“‹ DescripciÃ³n

ImplementaciÃ³n clÃ¡sica del entorno **Frozen Lake** para demostrar Aprendizaje por Refuerzo, especialmente los algoritmos SARSA y Q-Learning.

### CaracterÃ­sticas:
- âœ… **Agujeros (Holes)**: Terminan el episodio con recompensa -1
- âœ… **Comportamiento Slippery (Resbaladizo)**: Opcional, movimiento no determinista
- âœ… **Sistema de recompensas clÃ¡sico**: +1 objetivo, -1 agujero, 0 otro caso
- âœ… **Q-Learning y SARSA**: Ambos algoritmos implementados
- âœ… **VisualizaciÃ³n clara**: Hielo azul, agujeros negros, objetivo verde

---

## ğŸ“‚ Archivos

- `index.html` - Interfaz web principal
- `training.js` - LÃ³gica de entrenamiento
- `env_2D.js` - Entorno Frozen Lake
- `agentesRL.js` - Agente RL (Q-Learning y SARSA)

---

## ğŸš€ CÃ³mo Usar

### 1. Abrir la interfaz
```
Abre: index.html en tu navegador
```

### 2. Configurar parÃ¡metros
- **Alpha (Î±)**: 0.1 (tasa de aprendizaje)
- **Gamma (Î³)**: 0.9 (factor de descuento)
- **Epsilon (Îµ)**: 0.1 (exploraciÃ³n)
- **Episodios**: 1000-2000 (Frozen Lake necesita mÃ¡s episodios)
- **Agujeros (%)**: 15-25% (recomendado)
- **Slippery**: Activar para comportamiento no determinista

### 3. Crear entorno
- Click en "Crear Entorno"
- Se generarÃ¡ un grid con hielo, agujeros y objetivo

### 4. Entrenar
- Selecciona algoritmo (Q-Learning o SARSA)
- Click en "Entrenar"
- Observa cÃ³mo el agente aprende a evitar agujeros

### 5. Probar
- Click en "Probar Agente"
- Observa cÃ³mo navega evitando agujeros hacia el objetivo

---

## ğŸ¨ VisualizaciÃ³n

- ğŸŸ¦ **Azul claro**: Hielo seguro (celdas normales)
- ğŸ”µ **Azul oscuro con "S"**: PosiciÃ³n inicial (Start)
- âš« **Negro con borde rojo**: Agujeros (Holes) - terminan el episodio
- ğŸŸ¢ **Verde con "G"**: Objetivo (Goal)
- ğŸŸ¡ **CÃ­rculo dorado**: Agente

---

## ğŸ“Š Resultados Esperados

### Grid 8x8, Agujeros 20%, Determinista:
- **Episodios para converger**: ~500-1000
- **Tasa de Ã©xito**: 60-80% (depende de configuraciÃ³n de agujeros)
- **Recompensa promedio**: -0.5 a +0.5 (muchos episodios terminan en agujero)

### Grid 8x8, Agujeros 20%, Slippery:
- **Episodios para converger**: ~1000-2000
- **Tasa de Ã©xito**: 40-60% (mÃ¡s difÃ­cil con slippery)
- **Recompensa promedio**: -0.8 a +0.2

---

## ğŸ’¡ Conceptos Implementados

### Algoritmos:
1. **Q-Learning** (Off-Policy)
   - Aprende polÃ­tica Ã³ptima
   - Mejor para entornos deterministas
   - MÃ¡s agresivo en la exploraciÃ³n

2. **SARSA** (On-Policy)
   - Aprende polÃ­tica que sigue
   - Mejor para entornos no deterministas (slippery)
   - MÃ¡s conservador, evita riesgos

### Sistema de Recompensas (Frozen Lake):
- **+1**: Al alcanzar el objetivo (Goal)
- **-1**: Si cae en un agujero (Hole)
- **0**: En cualquier otro caso (hielo seguro)
- **Objetivo**: Maximizar recompensa aprendiendo a evitar agujeros

### Comportamiento Slippery (Resbaladizo):
Cuando estÃ¡ activado, el movimiento es **no determinista**:
- 33% probabilidad: AcciÃ³n intentada
- 33% probabilidad: AcciÃ³n perpendicular izquierda
- 33% probabilidad: AcciÃ³n perpendicular derecha

Ejemplo: Si intentas moverte a la derecha (â†’), puedes terminar:
- â†’ (33%): Derecha (intentada)
- â†‘ (33%): Arriba (perpendicular izquierda)
- â†“ (33%): Abajo (perpendicular derecha)

### PolÃ­tica:
- **Epsilon-Greedy**: Balance exploraciÃ³n/explotaciÃ³n
- **Frozen despuÃ©s del entrenamiento**: Usa polÃ­tica aprendida sin exploraciÃ³n

---

## ğŸ†š ComparaciÃ³n: Determinista vs Slippery

| Aspecto | Determinista | Slippery |
|--------|--------------|----------|
| **Dificultad** | ğŸŸ¡ Media | ğŸ”´ Alta |
| **Episodios necesarios** | ~500-1000 | ~1000-2000 |
| **Tasa de Ã©xito** | 60-80% | 40-60% |
| **Mejor algoritmo** | Q-Learning | SARSA |
| **Comportamiento** | Predecible | No determinista |

---

## ğŸ¯ CaracterÃ­sticas del Entorno Frozen Lake

### Elementos:
1. **Estados (S)**: Posiciones del agente en la cuadrÃ­cula (grid)
2. **Acciones (A)**: 4 direcciones (â†‘ â†“ â† â†’)
3. **DesafÃ­o**: Entorno no determinista (slippery opcional)
4. **Recompensas**:
   - +1: Alcanzar objetivo
   - -1: Caer en agujero
   - 0: Hielo seguro

### Diferencia con otros entornos:
- **No hay obstÃ¡culos que bloqueen**: Los agujeros terminan el episodio
- **Recompensas escasas**: Solo al final del episodio (+1 o -1)
- **Comportamiento no determinista**: Con slippery activado

---

## ğŸ”¬ Experimentos Sugeridos

1. **Comparar Q-Learning vs SARSA**:
   - Entrena ambos con slippery activado
   - SARSA deberÃ­a rendir mejor (mÃ¡s conservador)

2. **Efecto del porcentaje de agujeros**:
   - Prueba 10%, 20%, 30%
   - Observa cÃ³mo cambia la tasa de Ã©xito

3. **Determinista vs Slippery**:
   - Mismo grid, primero determinista, luego slippery
   - Compara tasas de Ã©xito y tiempo de convergencia

4. **TamaÃ±o del grid**:
   - Prueba 4x4, 8x8, 12x12
   - Observa cÃ³mo aumenta la dificultad

---

## ğŸ› Debugging

### Si el agente no aprende:
1. Aumenta episodios (Frozen Lake necesita mÃ¡s)
2. Reduce porcentaje de agujeros (empieza con 10-15%)
3. Aumenta epsilon (mÃ¡s exploraciÃ³n)
4. Prueba con grid mÃ¡s pequeÃ±o (4x4 o 6x6)

### Si siempre cae en agujeros:
- âœ… Normal al principio del entrenamiento
- El agente debe explorar para aprender
- Aumenta episodios de entrenamiento

### Si con slippery no converge:
- âœ… Slippery es mÃ¡s difÃ­cil
- Aumenta significativamente los episodios (2000+)
- SARSA funciona mejor que Q-Learning con slippery

---

## ğŸ“š Referencias TeÃ³ricas

### Q-Learning (Frozen Lake):
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
```
Donde `r = +1` si alcanza objetivo, `r = -1` si cae en agujero, `r = 0` otro caso

### SARSA (Frozen Lake):
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·Q(s',a') - Q(s,a)]
```
MÃ¡s conservador, aprende polÃ­tica real (importante con slippery)

---

## âœ… Estado del Proyecto

- [x] Entorno Frozen Lake completo
- [x] Agujeros que terminan episodio con -1
- [x] Comportamiento slippery (no determinista)
- [x] Sistema de recompensas clÃ¡sico (+1, -1, 0)
- [x] VisualizaciÃ³n clara (hielo, agujeros, objetivo)
- [x] Q-Learning y SARSA implementados
- [x] Interfaz web completa
- [x] DocumentaciÃ³n completa

---

**Â¡Perfecto para aprender los fundamentos de RL con un problema clÃ¡sico!** â„ï¸ğŸ¤–

**VersiÃ³n**: 1.0  
**Ãšltima actualizaciÃ³n**: Diciembre 2024

# ğŸ¯ Goal-Conditioned Reinforcement Learning

## ğŸš€ Â¿QuÃ© es Goal-Conditioned RL?

**Goal-Conditioned RL** es una tÃ©cnica donde el agente aprende a navegar hacia **cualquier objetivo** que le des, no solo hacia uno fijo. DespuÃ©s del entrenamiento, puedes **cambiar el objetivo dinÃ¡micamente** sin necesidad de re-entrenar.

---

## ğŸ†š ComparaciÃ³n de Enfoques

### 1. **Single-Goal** (Original)
```
Entrenamiento: Meta fija en (9, 9)
   â”œâ”€ Agente aprende: "Siempre ir a (9, 9)"
   â””â”€ DespuÃ©s: Solo puede ir a (9, 9)

âŒ Problema: No se adapta a nuevos objetivos
```

### 2. **Multi-Goal** 
```
Entrenamiento: 3 metas aleatorias
   â”œâ”€ Agente aprende: "Ir a la meta mÃ¡s cercana"
   â””â”€ DespuÃ©s: Va a una de las 3 metas conocidas

âš ï¸ LimitaciÃ³n: Solo conoce las metas del entrenamiento
```

### 3. **Goal-Conditioned** â­
```
Entrenamiento: MUCHAS metas aleatorias (cambian cada episodio)
   â”œâ”€ Agente aprende: "Navegar hacia CUALQUIER posiciÃ³n"
   â””â”€ DespuÃ©s: Puedes dar CUALQUIER objetivo nuevo

âœ… Ventaja: Generaliza a cualquier meta, incluso no vista
```

---

## ğŸ§  Â¿CÃ³mo Funciona?

### Arquitectura Clave

El agente usa una **Q-Table consciente del objetivo**:

```javascript
Q[state_y][state_x][goal_direction][action]
```

Donde `goal_direction` codifica la direcciÃ³n relativa al objetivo:
- 0: Arriba-Izquierda
- 1: Arriba
- 2: Arriba-Derecha
- 3: Izquierda
- 4: Mismo lugar
- 5: Derecha
- 6: Abajo-Izquierda
- 7: Abajo
- 8: Abajo-Derecha

### Durante el Entrenamiento

```javascript
// Cada episodio tiene un objetivo DIFERENTE
for (episode = 0; episode < numEpisodes; episode++) {
    env.randomizeGoal(); // ğŸ¯ Objetivo aleatorio
    // Agente aprende a navegar hacia ESE objetivo
}
```

El agente aprende:
- "Si estoy en (2,3) y el objetivo estÃ¡ a mi derecha-abajo, debo moverme derecha"
- "Si estoy en (5,5) y el objetivo estÃ¡ arriba, debo moverme arriba"
- **Generaliza** el concepto de "navegar hacia un objetivo"

### DespuÃ©s del Entrenamiento

```javascript
// Puedes cambiar el objetivo sin re-entrenar
env.setGoal([7, 2]); // Nuevo objetivo
agent.navigateToGoal([7, 2]); // âœ… Funciona!
```

---

## ğŸ“‚ Archivos Creados

### ğŸ†• Sistema Goal-Conditioned:

1. **`env_2DGoalConditioned.js`**
   - MÃ©todo `setGoal(newGoal)` - Cambiar objetivo dinÃ¡micamente
   - MÃ©todo `randomizeGoal()` - Objetivo aleatorio para entrenamiento
   - VisualizaciÃ³n interactiva con hover

2. **`agentesRLGoalConditioned.js`**
   - Q-Table con conciencia del objetivo
   - MÃ©todo `navigateToGoal(goal)` - Ir a objetivo especÃ­fico
   - Entrenamiento con mÃºltiples objetivos

3. **`trainingGoalConditioned.js`**
   - Click en canvas para cambiar objetivo
   - NavegaciÃ³n automÃ¡tica al hacer click
   - Callbacks para visualizaciÃ³n

4. **`trainingGoalConditioned.html`**
   - Interfaz interactiva
   - Badge "DYNAMIC GOAL"
   - Instrucciones visuales

---

## ğŸ® CÃ³mo Usar

### Paso 1: Abrir la Interfaz
```
Archivo: trainingGoalConditioned.html
```

### Paso 2: Configurar
- **Episodios**: 1500+ (necesita mÃ¡s que single-goal)
- **Grid**: 10x10
- **ObstÃ¡culos**: 10%

### Paso 3: Entrenar
1. Click en "Crear Entorno"
2. Click en "Entrenar"
3. Observa cÃ³mo cambia el objetivo en cada episodio

### Paso 4: Probar Interactivamente ğŸ¯
**Â¡AQUÃ ESTÃ LA MAGIA!**

1. DespuÃ©s del entrenamiento, **haz CLICK en cualquier celda del grid**
2. El agente **navegarÃ¡ automÃ¡ticamente** hacia ese punto
3. Prueba con diferentes objetivos
4. Â¡No necesitas re-entrenar!

---

## ğŸ¨ Interfaz Interactiva

### VisualizaciÃ³n:
- â­ **Estrella dorada**: Objetivo actual
- ğŸŸ¦ **CÃ­rculo azul**: Agente
- âš« **CÃ­rculos negros**: ObstÃ¡culos
- ğŸ“ **LÃ­nea punteada**: Camino directo (Manhattan)

### Interactividad:
- ğŸ–±ï¸ **Hover**: El canvas brilla
- ğŸ–±ï¸ **Click**: Establece nuevo objetivo
- âš¡ **AutomÃ¡tico**: El agente navega inmediatamente

---

## ğŸ”¬ Conceptos TÃ©cnicos

### 1. Goal Representation

```javascript
_getGoalDirection(state, goal) {
    const dy = goal[0] - state[0];
    const dx = goal[1] - state[1];
    
    // Convierte a 9 direcciones
    const dirY = dy === 0 ? 1 : (dy > 0 ? 2 : 0);
    const dirX = dx === 0 ? 1 : (dx > 0 ? 2 : 0);
    
    return dirY * 3 + dirX; // 0-8
}
```

### 2. Recompensa Basada en Distancia

```javascript
_getDistanceReward() {
    const dx = Math.abs(this.state[1] - this.goal[1]);
    const dy = Math.abs(this.state[0] - this.goal[0]);
    const distance = dx + dy;
    
    return -distance * 0.1; // MÃ¡s cerca = mejor
}
```

### 3. Entrenamiento Multi-Objetivo

```javascript
for (let episode = 0; episode < numEpisodes; episode++) {
    env.reset(true); // true = randomize goal âœ¨
    // ... entrenar hacia ese objetivo especÃ­fico
}
```

---

## ğŸ“Š Resultados Esperados

### ConfiguraciÃ³n de Prueba:
- Grid: 10x10
- ObstÃ¡culos: 10%
- Episodios: 1500
- Alpha: 0.1, Gamma: 0.9, Epsilon: 0.1

### MÃ©tricas:

| MÃ©trica | Single-Goal | Multi-Goal | Goal-Conditioned |
|---------|-------------|------------|------------------|
| **Convergencia** | 300 eps | 500 eps | 700 eps |
| **Flexibilidad** | âŒ Ninguna | âš ï¸ Limitada | âœ… Total |
| **Objetivos** | 1 fijo | 3-8 fijos | âˆ dinÃ¡micos |
| **Re-entrenar** | âœ… Necesario | âœ… Necesario | âŒ No necesario |
| **GeneralizaciÃ³n** | Baja | Media | Alta |

---

## ğŸ¯ Casos de Uso

### âœ… Ideal Para:

1. **NavegaciÃ³n DinÃ¡mica**
   - Usuario selecciona destino en tiempo real
   - Waypoints cambiantes

2. **RobÃ³tica**
   - Robot que debe ir a diferentes estaciones
   - Trayectorias que cambian segÃºn contexto

3. **Videojuegos**
   - NPCs que navegan a objetivos dinÃ¡micos
   - Pathfinding adaptativo

4. **ExploraciÃ³n**
   - Agente que explora puntos de interÃ©s
   - Objetivos determinados por sensores

### âŒ No Ideal Para:

- Tareas con meta Ãºnica conocida
- Cuando re-entrenar es barato
- Problemas donde la meta define la estrategia completa

---

## ğŸ’¡ Ventajas vs Desventajas

### âœ… Ventajas:

1. **Flexibilidad Total**
   - Cambia objetivos sin re-entrenar
   - Adapta a nuevos escenarios

2. **GeneralizaciÃ³n**
   - Aprende concepto general de navegaciÃ³n
   - Funciona con metas no vistas

3. **Eficiencia Post-Entrenamiento**
   - Una sola polÃ­tica para todos los objetivos
   - Deployment simplificado

4. **Interactividad**
   - Usuario puede interactuar en tiempo real
   - DemostraciÃ³n mÃ¡s impresionante

### âš ï¸ Desventajas:

1. **Entrenamiento MÃ¡s Largo**
   - Necesita ~2x mÃ¡s episodios
   - MÃ¡s complejo de optimizar

2. **Q-Table MÃ¡s Grande**
   - 9x mÃ¡s grande (por las direcciones)
   - Mayor uso de memoria

3. **Puede Ser Overkill**
   - Si solo necesitas 1-2 objetivos, es excesivo

---

## ğŸ”§ PersonalizaciÃ³n

### Cambiar Sistema de Direcciones

Actualmente usa 9 direcciones. PodrÃ­as usar 4:

```javascript
_getGoalDirection(state, goal) {
    const dy = goal[0] - state[0];
    const dx = goal[1] - state[1];
    
    // 4 direcciones cardinales
    if (Math.abs(dy) > Math.abs(dx)) {
        return dy > 0 ? 1 : 0; // Arriba/Abajo
    } else {
        return dx > 0 ? 3 : 2; // Derecha/Izquierda
    }
}
```

### Cambiar Sistema de Recompensas

Actualmente usa distancia Manhattan. PodrÃ­as usar Euclidiana:

```javascript
_getDistanceReward() {
    const dx = this.state[1] - this.goal[1];
    const dy = this.state[0] - this.goal[0];
    const distance = Math.sqrt(dx*dx + dy*dy);
    return -distance * 0.1;
}
```

---

## ğŸ§ª Experimentos Sugeridos

### 1. ComparaciÃ³n Directa
```
1. Entrenar single-goal en (9,9)
2. Entrenar goal-conditioned
3. Ambos navegar a (9,9)
4. Cambiar objetivo a (3,5)
   - Single-goal: âŒ Falla
   - Goal-conditioned: âœ… Funciona
```

### 2. Test de GeneralizaciÃ³n
```
1. Entrenar con obstÃ¡culos al 10%
2. Cambiar obstÃ¡culos al 15%
3. Probar con nuevos objetivos
4. Medir tasa de Ã©xito
```

### 3. Velocidad de AdaptaciÃ³n
```
1. Entrenar
2. Cambiar objetivo cada 5 segundos
3. Medir tiempo hasta llegar
4. Comparar consistencia
```

---

## ğŸ“š TeorÃ­a Subyacente

### Universal Value Function Approximators (UVFA)

Goal-Conditioned RL implementa una forma de UVFA:

```
V(s, g) = "Valor de estar en estado s con objetivo g"
Q(s, a, g) = "Valor de acciÃ³n a en estado s para objetivo g"
```

### Hindsight Experience Replay (HER)

Aunque no implementado aquÃ­, HER es una extensiÃ³n natural:
- Cada experiencia fallida se re-etiqueta como Ã©xito para otro objetivo
- Aprende incluso de fallos

### Transfer Learning

Goal-Conditioned permite transfer learning:
- Aprende en un entorno
- Transfiere a entornos similares con nuevos objetivos

---

## ğŸ“ Para Aprender MÃ¡s

### Papers Importantes:
1. "Universal Value Function Approximators" (Schaul et al., 2015)
2. "Hindsight Experience Replay" (Andrychowicz et al., 2017)
3. "Goal-Conditioned Reinforcement Learning" (Pong et al., 2018)

### Aplicaciones Reales:
- RobÃ³tica (Fetch, UR5)
- ManipulaciÃ³n de objetos
- NavegaciÃ³n autÃ³noma
- Juegos (AlphaStar, OpenAI Five)

---

## âœ… Resumen Ejecutivo

**Goal-Conditioned RL** es la evoluciÃ³n natural de RL cuando necesitas:
- ğŸ¯ **Flexibilidad**: Objetivos dinÃ¡micos
- ğŸ§  **GeneralizaciÃ³n**: Funciona con metas no vistas
- âš¡ **Rapidez**: Sin re-entrenamiento
- ğŸ® **Interactividad**: Usuario controla objetivos

**Trade-off**: Entrenamiento mÃ¡s largo, pero deployment infinitamente mÃ¡s flexible.

**Uso Recomendado**: Cuando los objetivos varÃ­an frecuentemente o son definidos por usuarios/entorno en tiempo real.

---

Â¡Experimenta haciendo click en el grid despuÃ©s del entrenamiento! ğŸ¯âœ¨

**Autor**: Sistema de RL Avanzado  
**VersiÃ³n**: 1.0  
**Fecha**: 2024

